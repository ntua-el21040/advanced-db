{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c644d2d-55fb-43ab-acd9-27cf33cc1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1446</td><td>application_1765289937462_1434</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1434/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-55.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1434_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66242d24e5aa49ba98ca78c89bcf4397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive'}, 'driverMemory': '2G', 'executorMemory': '2G', 'executorCores': 1, 'numExecutors': 4, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1429</td><td>application_1765289937462_1417</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1417/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1417_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1436</td><td>application_1765289937462_1424</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1424/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1424_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1437</td><td>application_1765289937462_1425</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1425/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1425_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1438</td><td>application_1765289937462_1426</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1426/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1426_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1441</td><td>application_1765289937462_1429</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1429/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1429_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1445</td><td>application_1765289937462_1433</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1433/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1433_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1446</td><td>application_1765289937462_1434</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1434/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-55.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1434_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"driverMemory\": \"2G\",\n",
    "    \"executorMemory\": \"2G\",\n",
    "    \"executorCores\": 1,\n",
    "    \"numExecutors\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1725a46b-a2fc-4ed4-a8fd-35df30d68e79",
   "metadata": {
    "editable": true,
    "id": "2uaG6GoZthpT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1031313e4e8f41e1bd86c7f29cf0b26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"query2_python\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d59fc1-2ce9-4e8f-9352-543ebff16526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39bb69fcc854e6796b64c0ed6148ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_old_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "crimes_new_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "\n",
    "\n",
    "descent_diction = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\",\\\n",
    "    header=True,\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd58d9cd-3028-4319-9206-d8744ebbddbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475c5df4e86f465cbe126a7eeff2ff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up 1: 2.5884 sec\n",
      "Warm-up 2: 2.5219 sec\n",
      "Warm-up 3: 2.7971 sec\n",
      "Run 1: 3.1966 sec\n",
      "Run 2: 2.6676 sec\n",
      "Run 3: 2.8649 sec\n",
      "Run 4: 2.7281 sec\n",
      "Run 5: 2.7901 sec\n",
      "Run 6: 2.8636 sec\n",
      "Run 7: 2.3575 sec\n",
      "Run 8: 2.3083 sec\n",
      "Run 9: 2.3265 sec\n",
      "Run 10: 2.4744 sec\n",
      "Mean time (excluding warm-ups): 2.6578 sec"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import time, gc\n",
    "\n",
    "# Number of warm-up executions and measured runs\n",
    "warmups = 3\n",
    "runs = 10\n",
    "times = []\n",
    "\n",
    "# Execute the same pipeline multiple times\n",
    "for i in range(warmups + runs):\n",
    "\n",
    "    # Merge old and new crime datasets into a single DataFrame\n",
    "    crimes_df = crimes_old_df.unionByName(crimes_new_df)\n",
    "\n",
    "    # Extract the year of occurrence from the date field\n",
    "    year_df = crimes_df.withColumn(\n",
    "        \"year\",\n",
    "        F.year(F.to_timestamp(F.col(\"DATE OCC\"), \"yyyy MMM dd hh:mm:ss a\"))\n",
    "    )\n",
    "\n",
    "    # Keep only records with valid year and victim descent information\n",
    "    clean_df = year_df.filter(\n",
    "        F.col(\"year\").isNotNull() & F.col(\"Vict Descent\").isNotNull()\n",
    "    )\n",
    "\n",
    "    # Count the number of crimes per year and victim descent category\n",
    "    per_race = (\n",
    "        clean_df.groupBy(\"year\", \"Vict Descent\")\n",
    "        .agg(F.count(\"*\").alias(\"cnt\"))\n",
    "    )\n",
    "\n",
    "    # Compute the total number of crimes per year using a window function\n",
    "    w_year = Window.partitionBy(\"year\")\n",
    "    ranked = per_race.withColumn(\"year_total\", F.sum(\"cnt\").over(w_year))\n",
    "\n",
    "    # Calculate the percentage contribution of each category per year\n",
    "    ranked = ranked.withColumn(\n",
    "        \"percent\",\n",
    "        F.round(F.col(\"cnt\") / F.col(\"year_total\") * 100, 1)\n",
    "    )\n",
    "\n",
    "    # Rank victim descent categories within each year by descending crime count\n",
    "    w_rank = Window.partitionBy(\"year\").orderBy(F.desc(\"cnt\"))\n",
    "    ranked = ranked.withColumn(\"rank\", F.dense_rank().over(w_rank))\n",
    "\n",
    "    # Select only the top 3 categories per year\n",
    "    top3 = ranked.filter(F.col(\"rank\") <= 3)\n",
    "\n",
    "    # Join with the dictionary table to get the full description of victim descent\n",
    "    result = top3.join(descent_diction, on=\"Vict Descent\", how=\"left\")\n",
    "\n",
    "    # Select and format the final output columns\n",
    "    result = (\n",
    "        result.select(\n",
    "            \"year\",\n",
    "            F.col(\"Vict Descent Full\").alias(\"Vict_Descent\"),\n",
    "            \"cnt\",\n",
    "            \"percent\"\n",
    "        )\n",
    "        .orderBy(F.desc(\"year\"), F.desc(\"cnt\"))\n",
    "    )\n",
    "\n",
    "    # Trigger execution and measure execution time\n",
    "    start = time.time()\n",
    "    rows = result.collect()\n",
    "    dt = time.time() - start\n",
    "\n",
    "    # Free memory explicitly after each run\n",
    "    del rows\n",
    "    gc.collect()\n",
    "\n",
    "    # Distinguish warm-up runs from measured runs\n",
    "    if i < warmups:\n",
    "        print(f\"Warm-up {i+1}: {dt:.4f} sec\")\n",
    "    else:\n",
    "        times.append(dt)\n",
    "        print(f\"Run {i-warmups+1}: {dt:.4f} sec\")\n",
    "\n",
    "# Compute the mean execution time excluding warm-up runs\n",
    "mean_time = sum(times) / len(times)\n",
    "print(f\"Mean time (excluding warm-ups): {mean_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a8d6280-1787-49d0-b26f-99a2f144d139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1e3c04564242ddac9c9bccfec1f2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up 1: 3.5139 sec\n",
      "Warm-up 2: 2.8723 sec\n",
      "Warm-up 3: 2.9223 sec\n",
      "Run 1: 2.6963 sec\n",
      "Run 2: 2.4882 sec\n",
      "Run 3: 2.7207 sec\n",
      "Run 4: 2.6706 sec\n",
      "Run 5: 2.6070 sec\n",
      "Run 6: 2.8759 sec\n",
      "Run 7: 3.0987 sec\n",
      "Run 8: 3.3833 sec\n",
      "Run 9: 2.7290 sec\n",
      "Run 10: 2.5429 sec\n",
      "Mean time (excluding warm-ups): 2.7813 sec"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import time, gc\n",
    "\n",
    "warmups = 3\n",
    "runs = 10\n",
    "times = []\n",
    "\n",
    "\n",
    "crimes_old_df.unionByName(crimes_new_df).createOrReplaceTempView(\"crimes\")\n",
    "descent_diction.createOrReplaceTempView(\"descent_diction\")\n",
    "\n",
    "query = \"\"\"\n",
    "WITH year_df AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        year(to_timestamp(`DATE OCC`, 'yyyy MMM dd hh:mm:ss a')) AS year\n",
    "    FROM crimes\n",
    "),\n",
    "clean_df AS (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM year_df\n",
    "    WHERE year IS NOT NULL\n",
    "      AND `Vict Descent` IS NOT NULL\n",
    "),\n",
    "per_race AS (\n",
    "    SELECT\n",
    "        year,\n",
    "        `Vict Descent`,\n",
    "        COUNT(*) AS cnt\n",
    "    FROM clean_df\n",
    "    GROUP BY year, `Vict Descent`\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT\n",
    "        year,\n",
    "        `Vict Descent`,\n",
    "        cnt,\n",
    "        SUM(cnt) OVER (PARTITION BY year) AS year_total,\n",
    "        ROUND(cnt / SUM(cnt) OVER (PARTITION BY year) * 100, 1) AS percent,\n",
    "        DENSE_RANK() OVER (PARTITION BY year ORDER BY cnt DESC) AS rank\n",
    "    FROM per_race\n",
    "),\n",
    "top3 AS (\n",
    "    SELECT *\n",
    "    FROM ranked\n",
    "    WHERE rank <= 3\n",
    "),\n",
    "final AS (\n",
    "    SELECT\n",
    "        t.year,\n",
    "        d.`Vict Descent Full` AS Vict_Descent,\n",
    "        t.cnt,\n",
    "        t.percent\n",
    "    FROM top3 t\n",
    "    LEFT JOIN descent_diction d\n",
    "      ON t.`Vict Descent` = d.`Vict Descent`\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM final\n",
    "ORDER BY year DESC, cnt DESC\n",
    "\"\"\"\n",
    "\n",
    "for i in range(warmups + runs):\n",
    "    \n",
    "    result_sql = spark.sql(query)\n",
    "\n",
    "    start = time.time()\n",
    "    rows = result_sql.collect()\n",
    "    dt = time.time() - start\n",
    "\n",
    "    del rows\n",
    "    gc.collect()\n",
    "\n",
    "    if i < warmups:\n",
    "        print(f\"Warm-up {i+1}: {dt:.4f} sec\")\n",
    "    else:\n",
    "        times.append(dt)\n",
    "        print(f\"Run {i-warmups+1}: {dt:.4f} sec\")\n",
    "\n",
    "mean_time = sum(times) / len(times)\n",
    "print(f\"Mean time (excluding warm-ups): {mean_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e86d7-68a3-4b32-ad7d-2d2b23ce5501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
